{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q langchain-google-genai langchain-qdrant langchain-text-splitters langchain-community langgraph langchain\n",
    "## there casn be issues with versions, I struggled a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"False\"  ##true does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b271fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from pydantic import ConfigDict\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a92105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaac8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MY_API_KEY = \"AIzaSyBYCquvMp6UAaODqfjRu0lppEUUwIa4foA\"\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\"\n",
    "                                          , google_api_key=MY_API_KEY)\n",
    "ex_emb = embeddings.embed_query(\"sample text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install moviepy librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7cee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from moviepy import *\n",
    "\n",
    "# video_path = \"../2 part Databases for GenAI.mp4\"\n",
    "# audio_path = \"../extracted_audio.mp3\"\n",
    "\n",
    "# # Load the video file\n",
    "# video = VideoFileClip(video_path)\n",
    "\n",
    "# # Extract the audio and write it to a new file\n",
    "# video.audio.write_audiofile(audio_path)\n",
    "\n",
    "# import torch\n",
    "# from transformers import pipeline\n",
    "# model_id = \"distil-whisper/distil-small.en\"\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "# transcriber = pipeline(\n",
    "#     \"automatic-speech-recognition\",\n",
    "#     model=model_id,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "# import librosa\n",
    "# audio_array, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "# transcription = transcriber(audio_array, return_timestamps=True)['text']\n",
    "\n",
    "# with open(\"../audio_transcript.txt\", \"w\") as text_file:\n",
    "#     text_file.write(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02281d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"../audio_transcript.txt\") ## or maybe something else, txt, doc or pdf without pictures\n",
    "transcript = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85f06e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "vector_size = len(ex_emb)\n",
    "\n",
    "if not client.collection_exists(\"test\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"test\",\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "    )\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"test\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c312b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\"\n",
    "                      , google_api_key=MY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a7776d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(transcript)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a731832",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRetriever(BaseRetriever):\n",
    "    vector_store: object\n",
    "    \n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def __init__(self, vector_store: object):\n",
    "        super().__init__(\n",
    "            vector_store=vector_store\n",
    "        )\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Translates a string query into a graph invocation and returns documents.\n",
    "        \"\"\"\n",
    "        # print(f\"\\n[StateGraphRetriever]: Processing query: {query}\")\n",
    "        \n",
    "        # 1. CONSTRUCT THE INPUT STATE\n",
    "        initial_state = {\n",
    "            \"question\":  query,\n",
    "             \"context\": \"\", \n",
    "        }\n",
    "\n",
    "        # 2. INVOKE THE GRAPH\n",
    "        # We run the graph to completion\n",
    "        final_state = self.vector_store.similarity_search(initial_state[\"question\"])\n",
    "\n",
    "        return final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd42aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_retriever = TextRetriever(\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8593a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = text_retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \n",
    "                              \"context\": state[\"context\"]}) \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0ded4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_chat(question: str):\n",
    "    res = graph.invoke({\"question\": question})\n",
    "    return res[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e48bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Which maximal accuracy percent was reported for legal research?\n",
      "The maximal accuracy percent reported for legal research is 87%. This was achieved using 1.4 million law documents with Gemini embedding.\n"
     ]
    }
   ],
   "source": [
    "print(my_chat(\"Which maximal accuracy percent was reported for legal research?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
