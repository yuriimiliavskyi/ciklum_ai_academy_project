{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb7843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm-:--:--\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting moviepy\n",
      "  Downloading moviepy-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting more-itertools (from openai-whisper)\n",
      "  Downloading more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting numba (from openai-whisper)\n",
      "  Downloading numba-0.62.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai-whisper) (2.3.4)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Downloading tiktoken-0.12.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai-whisper) (2.7.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from moviepy) (5.2.1)\n",
      "Collecting imageio<3.0,>=2.5 (from moviepy)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n",
      "  Downloading imageio_ffmpeg-0.6.0-py3-none-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting proglog<=1.0.0 (from moviepy)\n",
      "  Downloading proglog-0.1.12-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting python-dotenv>=0.10 (from moviepy)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pillow<12.0,>=9.2.0 (from moviepy)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba->openai-whisper)\n",
      "  Downloading llvmlite-0.45.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tiktoken->openai-whisper) (2025.10.23)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tiktoken->openai-whisper) (2.32.5)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch->openai-whisper) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch->openai-whisper) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch->openai-whisper) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch->openai-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch->openai-whisper) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch->openai-whisper) (2025.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
      "Downloading moviepy-2.2.1-py3-none-any.whl (129 kB)\n",
      "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Downloading imageio_ffmpeg-0.6.0-py3-none-macosx_11_0_arm64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading proglog-0.1.12-py3-none-any.whl (6.3 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "Downloading numba-0.62.1-cp313-cp313-macosx_12_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.12.0-cp313-cp313-macosx_11_0_arm64.whl (993 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.45.1-cp313-cp313-macosx_12_0_arm64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=ab8511b1f632f8f543ac3a4cf7304f67605e55c171783c107a6bc1385b1a141e\n",
      "  Stored in directory: /Users/yurii/Library/Caches/pip/wheels/ca/58/d5/fb4539ad74c3ca81eb40f7eda020ac77d080b33ad57449d485\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: python-dotenv, proglog, pillow, more-itertools, llvmlite, imageio_ffmpeg, tiktoken, numba, imageio, openai-whisper, moviepy\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 12.0.0\n",
      "    Uninstalling pillow-12.0.0:\n",
      "      Successfully uninstalled pillow-12.0.0\n",
      "Successfully installed imageio-2.37.0 imageio_ffmpeg-0.6.0 llvmlite-0.45.1 more-itertools-10.8.0 moviepy-2.2.1 numba-0.62.1 openai-whisper-20250625 pillow-11.3.0 proglog-0.1.12 python-dotenv-1.2.1 tiktoken-0.12.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install openai-whisper moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6bfecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc77670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio from ../2 part Databases for GenAI.mp4...\n",
      "MoviePy - Writing audio in extracted_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted and saved to extracted_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from moviepy import *\n",
    "\n",
    "video_path = \"../2 part Databases for GenAI.mp4\"\n",
    "audio_path = \"../extracted_audio.mp3\"\n",
    "\n",
    "# --- Part 1: Extract Audio using MoviePy ---\n",
    "print(f\"Extracting audio from {video_path}...\")\n",
    "\n",
    "# Load the video file\n",
    "video = VideoFileClip(video_path)\n",
    "\n",
    "# Extract the audio and write it to a new file\n",
    "video.audio.write_audiofile(audio_path)\n",
    "\n",
    "print(f\"Audio extracted and saved to {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9471b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Loading model...\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1018)>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSSLCertVerificationError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:1319\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1336\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1382\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1381\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1331\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1091\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1094\u001b[39m \n\u001b[32m   1095\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1035\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1477\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1475\u001b[39m     server_hostname = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m-> \u001b[39m\u001b[32m1477\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mSSLCertVerificationError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1018)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# This should now work, as it will not verify the SSL cert during download\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mwhisper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbase\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/whisper/__init__.py:137\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, device, download_root, in_memory)\u001b[39m\n\u001b[32m    134\u001b[39m     download_root = os.path.join(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mXDG_CACHE_HOME\u001b[39m\u001b[33m\"\u001b[39m, default), \u001b[33m\"\u001b[39m\u001b[33mwhisper\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _MODELS:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     checkpoint_file = \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MODELS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     alignment_heads = _ALIGNMENT_HEADS[name]\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m os.path.isfile(name):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/whisper/__init__.py:73\u001b[39m, in \u001b[36m_download\u001b[39m\u001b[34m(url, root, in_memory)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     69\u001b[39m         warnings.warn(\n\u001b[32m     70\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exists, but the SHA256 checksum does not match; re-downloading the file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m source, \u001b[38;5;28mopen\u001b[39m(download_target, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[32m     75\u001b[39m         total=\u001b[38;5;28mint\u001b[39m(source.info().get(\u001b[33m\"\u001b[39m\u001b[33mContent-Length\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m     76\u001b[39m         ncols=\u001b[32m80\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m         unit_divisor=\u001b[32m1024\u001b[39m,\n\u001b[32m     80\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, context)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:489\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    486\u001b[39m     req = meth(req)\n\u001b[32m    488\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    492\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:506\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    505\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    465\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:1367\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:1322\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1319\u001b[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[32m   1320\u001b[39m                   encode_chunked=req.has_header(\u001b[33m'\u001b[39m\u001b[33mTransfer-encoding\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1323\u001b[39m     r = h.getresponse()\n\u001b[32m   1324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[31mURLError\u001b[39m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1018)>"
     ]
    }
   ],
   "source": [
    "\n",
    "# # --- Part 2: Transcribe Audio using Whisper ---\n",
    "# print(\"Loading Whisper model...\")\n",
    "# import ssl\n",
    "# _create_unverified_https_context = ssl._create_unverified_context\n",
    "# ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# print(\"Loading model...\")\n",
    "# # This should now work, as it will not verify the SSL cert during download\n",
    "# model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec70a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c02e96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from librosa) (0.62.1)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from librosa) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from librosa) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from librosa) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from librosa) (5.2.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp312-abi3-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from librosa) (4.15.0)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting standard-aifc (from librosa)\n",
      "  Downloading standard_aifc-3.13.0-py3-none-any.whl.metadata (969 bytes)\n",
      "Collecting standard-sunau (from librosa)\n",
      "  Downloading standard_sunau-3.13.0-py3-none-any.whl.metadata (914 bytes)\n",
      "Requirement already satisfied: packaging in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from numba>=0.51.0->librosa) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from pooch>=1.1->librosa) (4.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pooch>=1.1->librosa) (2.32.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Collecting cffi>=1.0 (from soundfile>=0.12.1->librosa)\n",
      "  Downloading cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting standard-chunk (from standard-aifc->librosa)\n",
      "  Downloading standard_chunk-3.13.0-py3-none-any.whl.metadata (860 bytes)\n",
      "Collecting audioop-lts (from standard-aifc->librosa)\n",
      "  Downloading audioop_lts-0.2.2-cp313-abi3-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile>=0.12.1->librosa)\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.10.5)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.2-cp313-cp313-macosx_11_0_arm64.whl (84 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp312-abi3-macosx_11_0_arm64.whl (163 kB)\n",
      "Downloading standard_aifc-3.13.0-py3-none-any.whl (10 kB)\n",
      "Downloading standard_sunau-3.13.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl (181 kB)\n",
      "Downloading audioop_lts-0.2.2-cp313-abi3-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading standard_chunk-3.13.0-py3-none-any.whl (4.9 kB)\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: standard-chunk, soxr, pycparser, msgpack, lazy_loader, audioop-lts, standard-sunau, standard-aifc, pooch, cffi, soundfile, audioread, librosa\n",
      "Successfully installed audioop-lts-0.2.2 audioread-3.1.0 cffi-2.0.0 lazy_loader-0.4 librosa-0.11.0 msgpack-1.1.2 pooch-1.8.2 pycparser-2.23 soundfile-0.13.1 soxr-1.0.0 standard-aifc-3.13.0 standard-chunk-3.13.0 standard-sunau-3.13.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602455af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "audio_array, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "result = transcriber(audio_array, return_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa079a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" session we can skip few slides. So in this session we will focus again, we will reiterate a bit about the rock, and we will go a bit more deeper in embedings, vectorization and other rock systems and possibilities. And we should come to the state where we can even build quiet, big and reliable, and reliable a rock system that will utilize rock. At this, this slide I can skip. So we will focus on the foundation, then more advanced rock patterns, and some do's and ons for production readiness, and here we will cover even one of the question that we had for the previous session. So on the foundation, so what is in general like embedings, like embedding, like embedings, like embedings, like embeddings, like embeddings, like embeddings, like embeddings, like embeddings, like embeddings, like embeddings, like embeddings, like embeddings, like embeddings, like embeddings, like embeddings, like embeddings, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like have the problem with that, that examples like words, mention learning, algorithm, email, techniques, and deep neural networks, that's basically computer doesn't understand similarity of that words. Computers still understand only like numbers. So it cannot help us to define just from the simple text. Are they similar? So basically like embedings, it's a transformation of the text to the numeric representation on the vector. So when we talk about like machine learning, then for the, for our embedings, we have like this representation of the numbers depends on the dimensions. depends on the dimensions. So it's like on this exactly like image we have 536 dimensions. So it's basically like graphs with the different points and in a dimension of this size. We are putting the representation of that graph. the same like for the ML algorithm pizza and stuff like that. And if we are asking like about like machine learning, for example, then computer based on that vector representation and number representation can understand if like machine learning, for example, because we already the transformed the representation into the numbers, if it's if it's close to some of the like information that we have right now in the vector system, or it's far, so based on like close and far, it's basically like vector databases when we set up like choose top five results, providing to us with the answer from the five closest. objects and then retransform this to the text and we are basically getting that's that's text that most closes to to the text from our request. In this case, like for the machine learning, ML algorithm is quite close because the distance 0.0. 0.02. And if we just want to get only one like top element, then we are getting MLL grids. Pizza recipes is quite far. So most probably system will not suggest this to us. And if we are going a bit like deeper to the exact like similarity search, how it works. how it works. So we have our documents, we already discussed about the rock pipeline, so it's already transformed to the numerical representation from the embedding models and it stores somewhere here to the vector store, and we have like three documents and we fully indexed that documents and it's that documents and it's that documents and it's that documents and it's that documents and it's that documents and it's that documents and it's that documents and it's that documents and it's that documents and it's that documents and it's that documents and it that documents and it's that documents and it's that documents and it And when we have a search, and if we have like user query, query, learn Python, then based on that documents that we stored our system identified the distance for our request, and in this case, quiet close, and far away, if we set up that we want to pick up two top results, So in this case we will get Python tutorial and Java programming as well and then LLLM maybe make a decision that's to show only like Python tutorial but it can be the case that it will provide as a output to the user both like Python tutorial and Java programming because quality of the LLLM play quite significant control in this case as well. Quite often, like similarity metrics, that systems identify, this is angle between the actors, vector, sorry, straight line, just distance between them, and in some cases, some dot multiplication in that systems. So why is like embedding quality matters, because basically it's mean that how reliable answer, our user of our system will get? And right now, MTEB metrics. MTEB metrics, to be honest, I don't, I don't remember exact like this full abbreviation, but I provided the link to the leader board and you will see what exactly it's mean, what is the abbreviation, so this is the approach, how to evaluate how current embedding models evaluated on the quality of the quality of the quality of the quality of the quality of their basically embedings and accuracy of their embedings. And in the in the point of time when I was preparing this presentation a few of the main or like top embedding models basically from the Google and open AI and one of the open source knowledge but right now I guess Chinese embedding models on a good spot there as well so you can try and play. And why are they exactly like embedding quality matters if we will see the different case studies or researchers you will still see that any embedding model or embedding system can provide 100 percentage of the accuracy because still LLLM or like similarity search can be like mistakenly in interpreter and one of the like best result that you can find like on the Google website for example. For example, like best result that you can like on the Google website, for example, it's research about the legal discovery. So they put 1.4 million of different law documents with utilization of Gemini embedding, and they achieved 87% accuracy. And it's like for sure quite good number. number. And with another like application, Mindled, they achieved 82% of three top results recall, in terms of their metrics, it's as well, quite good result. And in which, what is good with the direction of embedding models, and in which what is good with the direction of embedding models, and we So just a bit when we discuss about chunking strategies, that's when you utilize like additional ML or Gen EI or embeding models, it's additional time for indexing, and it's additional time for the compute, so it's much slower. Jim and I embedding, already showing like quite good results in terms of the speed in terms of the speed. So they tried to vectorize 100 emails on one of the study and they achieved 21 and half seconds for vectorization of all of that emails. And just to give you a perspective of how it fast and what is the, how fast we are moving to improving all of... these tools and systems. The previous result was more than 200 seconds. So the order of magnitude in terms of the speed increase for the electrization in 10 times. So that system much more like improved. And we are going like on that speed, and that changes with... many models systems and tools in AI world because people experimenting companies putting a lot of effort in this race, so to say. And if we will talk a bit more about the databases, for the vectorization. This is where I wanted to mention about the speed of changes as well, because early I had this like slide, why we need to have like separate database for embedings and that's like current databases like systems. They have different algorithms. They have different algorithms. have everything that is needed for the embedings that they quiet slow for the embedings. And it was, but post-Gris is moving with their extension, they are moving in quite rapidly and catching up the speed question and especially amount of the operations, basically amount of the operations. basically like queries per second that they can handle and a lot of the researchers already show that postgress is quite good alternative to the specialized even data basis for the rock. So you can see that for example like faster than pine cone and for sure like cheaper and still open source and for quadrant for quadrant it can handle much more request per second but it's on a huge amount of the data so they tested this on the 50 million embedings. And this is where like quadrons start degrading with the with the speed of working. So potentially you can use just that database that you use that you used to and you just need to add additional extension for the embedings and you can continue playing with your lovely postgras. But still we we have the databases for the vectorization and they still play their own game and for what use case for what systems they they They have the best results and it was already mentioned about like Roma today and Roma is quite good when you are starting some of the prototyping. It's less rare used in some big production systems, but they already have their cloud Roma, I didn't try. And still they have some kind of like limitations. So, chroma database are quite good when you are starting and prototyping because it's quite easy to start. You just people install chroma and almost everything is working. So, minimal configuration. And you are starting almost like immediately with that. quite a good additional functionality, like for example, made that metadata filtering. This is the additional information to your embedings that you can provide. As an example, I've already mentioned, for example, like category of the document that you are feeding. If you have like many of the documents and chunks, for example, in the metadata, and chunks, for example, in the metadata, you can still additionally provide information about what exact document from which, from, from, from chunk is, and different, like multiple collections support. One we are moving already, like to the stage of potential MVP or already quite big So MVP, or already quite big production system, then we need to have more reliable solution. And here we can look in the direction of quadrant, or for example, like post-dress with the PG vector extension. And on the previous slide, you saw that quadrant, not so good on 50 million vectors in chance of chance. queries per second. And it's in the in the measure between like 1 to 10 million, it's really quite quite fast and can handle a lot of the requests. Like for example, for 1 million, it's 626 queries per second. it's really quiet fast. And for that, if you have that range of the vectors and you need to have quiet rapid embedding, quadrant is a good choice in general. But if you have like billions of vectors, then this is the type of the system. And Milwus, and they have this embedding database in a cloud it calls Zeline, it's already fully designed for quite huge vector systems. And it doesn't make sense to use this database for like much smaller systems. If you have under 10 million vectors, so do not recommend because do not recommend because that's that data and that systems that going from the left to right. It's then just harder, like more complex to support setup and configure. So that's why you are starting from the simplest one just for the rapid start and when your system is growing. just for the rapid start and when your system is growing you are moving to the let's say next one. And each of them just fulfilled their purples. And about like rock patterns, rock patterns in addition to the embedings because we discussed about the embedings vectorization. one pattern or approach for the rock systems. In addition, we can have this problem with the multi-hop reasoning, with the connection. And in the cases, where we need to, well, we have like a bit more like a request, and we need to identify the connection for that request. request in most cases, rock, traditional vector rock systems, they are failing and in example that I provided and the problem that we need to have like connection built from our request that user asks like marketing we need to have like connection built from our request that user ask like marketing budget compliance GDPR and basically Europe and this is the connections and this is exactly what we need already to utilize graph system and graph rock in addition and it helps to increase reliability of our system of our system. together with BLM. And quite often this is like a representation of the graph that we have. And when we, when our user making the request, it's just like getting some of the information from the request and learning the different connections. And based on the connection that we have in the rock representation, provides much better answer if we do not have like explicit information and one chunk of the information that we are grabbing from our vector system is not enough. Main players for the graph rack solutions, so now for J, Falcon, Tiger Graph, Mam Graph and Arongo DB. Mainly we are playing with the Mainly, we are playing like with the now for J because you can easily like install it's on your laptop. And I guess it's one of the most popular solutions I would say. have images in the PDF, we can have tables and maybe some formulas. So it's quiet, complex documents to to parse for the LLLM. And in the traditional rock pipelines, we have like OCR objects, so basically the ML system or it can be like LLLM, that can look exactly on the PDF and then provide in the text. in the text view describe what it sees on the PDF and then we can just like vectorize it quite easily. Still we have like issue with the tables quite often and here like some OCR again can help some other approaches. relying on OCR because they quite often like failing with that, we can just use some embedding model that can understand what it see show, just embed the image. And one of the interesting approach to solve this question for the last time. It's quite like new, let's say, approach. It's called Palli library changes. So they instead of like using OCR just for the describing what information exists on the PDF and then vectorized that information. They have like vision language language model. for that and image representation just split to the purchase and then that purchase basically embed to the model. And when we have the retrieval, then we utilize that embedding model and still like visual language model to, to give the answer, what we have from that images, and this approach shows much better results in terms of like rock system retrieval of the information from the system. And it showed better result like 15% approximately than standard retrieval system with the OCR that we have. So to answer on the question, quite often when we utilize the PDFs, when we have a PDFs as documents, the OCR for describing of that documents, and then we stored the information. But this approach was the Kolkali showing quite interesting results, and maybe it's something that will be used much more often in the future as a part of this type of frag systems. But still mostly OCR approach used. bit about like agentic rock because in AI right now everything changed to the agentic rock and for sure this is quite interesting and quite reliable approach. So in the standard rock we have query and that query go into the embedding model, then we query vector the embedding model, then we query the vector database, go into the embedding model, then we query vector databases data databases then back to the database is provide to us like candidates, like top candidates, and then our query so our request plus candidates that our vector database provided goes to the LLLM, LLLM, then protests all of that information that we put and then provides to our user the answer. type of like rotor agent LLLM's and then I was in the tool set that's available for that LLLM's, it's choose where to to roll that request. So to the rock, lab search, some external APIs or taking control over the world and then only providing the output message. Here we will have a bit more we will have a bit more information on that. So agentic patterns and maybe like query routing requests, some of the decomposition and self correction approaches in the agentic rack system. So when the user makes the requests, what the latest on AI regulation, so LLLAM rotor detect like latest, then we will utilize the two road to web search. So that's why I said that Like in general, if we are saying that we add just additional tool like web search, yes it's to some extent track system. And like when we talk about the query the composition, so we have compare our Q3 to industry and predict Q4, and first of all we have the decomposition of that request that agents, that agents, our LLM bricks like our Q3 matrix, then we need to find this information in our internal database or vector system. industry Q3 benchmarks, potentially we don't have this information, it's publicly available, then web search, and Q4 factors, again, goes to the internal DB, but again it can go to the web search as well, and the search pattern itself correction, so we first have retrieval, then we get in the documents, then our documents, then our agent based on some identified our like threshold measured this and if score is like lower than our threshold then it can even like rewrite the query and then basically like retry again this approach with the retrieval grade dogs. And it can be cycled a few times. And when it's it's to use well one we have like quiet complex worries and in cases if everything else that we discussed earlier failed and you need to have like more high accuracy. If we have one we shouldn't use this it's for the simple retrievals and if our top. quality attribute for our system is latency because you understand that this agentic systems where we have LLLM and needs to increase the quality of their results the time of these types of the request is growing. I guess we will need maybe Xana I guess our time is and yes we are a bit out of time. If you have a time you can continue and also colleagues if you have a little bit of 10 minutes if we can continue this session. I will try to finish this like in five minutes and then we will have five minutes for for the questions. I just will not stop quite deeply on each of the slide, just few words and what is the information important from that. In addition to increase the quality of our RAC systems and why I mentioned earlier that you shouldn't rely only on the vector search search. In addition, you can utilize like best much search. This is what BM25. It doesn't have semantic understanding, but it's calculated the number of words that it finds like the same words that it's find in the different documents and can provide based on that statistic statistical calculation, what the documents we should be On the researchers, when we have like a hybrid system approach in terms of the search vector plus this BM25, it's increased accuracy for a bit more than 10 percentage and DC G3, this is like top three recalls basically top three candidates that we saw earlier on the diagram and when we add the rerun care in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, in addition, it's even increased to 37.2 percentage, rerun its additional layer that we add into the system, and we are grabbing from our vector system or from our database a bit more results. So we are grabbing instead of like 510 candidates, we are grabbing 100 candidates, and then our rerunkin system them, try to understand what is the best candidates for us, and then provide as a result to the LLLM, like five or ten or maybe three of them. And production readiness, so basically what you do don't, hybrid search is the best. We discussed in addition about like, agentric search, we discussed about the graph and where it's the most useful and what databases you can utilize. So based on your case, based on what you need to achieve, you can use any of that tools. Metadata filtering. It's quite helpful, especially when you need to have metadata in general, have, like, metadata in general, like, storing of the metadata, quite helpful, especially when you need to have the resources seated, like, for example, when you make the request and get not only some information from your documents, in addition, the citations or from what documents that information is. Evolution of your system, embedding, re-indexing, and evaluation of the system quite often, because you are making the updates, and you can continue improving your system. So don't, it's like opposite of the do's. So just quite helpful for you to not forget what you should do for the production system. And thank you. So we have one question in our chat. Does embed in library I use for my rock must match embed in library I used for training my LLLM. So before training LLLM as I understand I need to embed like all the text and everything and do the embedding. Yeah. When you have a rock system you do not train your LLLM like fine-tuned your LLLM? Yeah, it's not about I mean like when I trained my LLLM I used some embedding for example from from Facebook or Google. Google, do I have to use the same embedding library for my rug? I mean, as I understand, like to put something into the rug, you'll also need to do embedding. Do you mean when you are storing the documents to the vector database and when you are retrieving the documents from the database, do you need to use the same embedding? to use the same. Yes. Yes. When it generates a no. As an answer, yes. Okay. And the version and everything should be the same and identical with LAM. Yes, sometimes especially like for one provider, they compatible one and another, but you need to double check this information. but general answer is a yes. Good, you are preparing in most cases, yes, because we have metrics like NGCG or recall, require ground truth and answers. Should they come from humans? on your request. So yeah, mainly it's human prepared metrics. Thank you, Maxim, colleagues, other questions, maybe you have. slides you mentioned that we have to evaluate often and I think this is the most complex task when it comes to building AI-based solutions. So my question is whether we'll have any session any session that will explain how to build these evaluations for our systems for overall like agi gendic systems. So the question is about evaluations. I will double check maybe in the next sessions that we will have on the rock enterprise productized, maybe we will have the session on evaluation, or we will discuss with our colleagues that we should potentially like add this as a part of our education because of our education because we should what I see I don't see like exact evaluation in in the session, but potentially it's a part of some of the next sessions. Okay, thank you. Why I said not only because preparation of the data, it's not so easy as well. Without evaluation, you just don't know whether you did good job at preparing your data or not. job at preparing your data or not. So you can be building a system for like weeks or months, but if you do not have any metrics to check its accuracy and performance, then it wasn't for nothing. So. As always, human in the loop can save you from that, but yes, depends on the scalability of your system and all of that parameters. Yes, I agree. But in general, what can I say, it's not so easy still question for the evaluation in general with within the work with all of this LLLM systems because they are like they are not like deterministic systems and still you need to identify the proper way how you can like even identify that tops read that system should recall. So different approaches in most the in most the approaches like some another LLLM utility. if you are not utilizing the people to prepare the data. So it's still not finalized question even for in general for the AI industry, I would say.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"text\"] ##save to txt or rtf!! (already done manually, TBD programmatically)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb117448",
   "metadata": {},
   "source": [
    "# RAG implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d838eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c03ff8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_AUTH_SUPPRESS_ADC_CREDENTIALS\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "833866f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "196ce90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"False\" ##true does not work\n",
    "# Langsmith_key = \"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\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(Langsmith_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d839106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q \"langchain[google-genai]\" langchain-google-genai langchain-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62154bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "#   os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"AIzaSyBYCquvMp6UAaODqfjRu0lppEUUwIa4foA\") ##does npt work later\n",
    "\n",
    "MY_API_KEY = \"AIzaSyBYCquvMp6UAaODqfjRu0lppEUUwIa4foA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266824ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\"\n",
    "                                          , google_api_key=MY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "603ac430",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_emb = embeddings.embed_query(\"sample text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ada6883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../audio_transcript.rtf\")\n",
    "transcript = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4519452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "vector_size = len(ex_emb)\n",
    "\n",
    "if not client.collection_exists(\"test\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"test\",\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "    )\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"test\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac8de436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\"\n",
    "                      , google_api_key=MY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06d60195",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(transcript)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5683f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d888007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "403ca178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": transcript[0].page_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4b832f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=b66b6478-4626-431e-83e5-b65e68e44657,id=b66b6478-4626-431e-83e5-b65e68e44657; trace=b66b6478-4626-431e-83e5-b65e68e44657,id=26519150-a86e-48a9-aa8b-0caa47ff2d3a\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=b66b6478-4626-431e-83e5-b65e68e44657,id=26519150-a86e-48a9-aa8b-0caa47ff2d3a; trace=b66b6478-4626-431e-83e5-b65e68e44657,id=cba8aaac-2786-4146-aab3-1cb9da99b3b3; trace=b66b6478-4626-431e-83e5-b65e68e44657,id=9849aaff-b414-4663-9358-2acec9cd9b6e; trace=b66b6478-4626-431e-83e5-b65e68e44657,id=9849aaff-b414-4663-9358-2acec9cd9b6e; trace=b66b6478-4626-431e-83e5-b65e68e44657,id=97d16771-da57-4ef1-afa0-2955d2669ceb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For legal research, a study utilizing Gemini embedding on 1.4 million law documents achieved 87% accuracy. Another application, Mindled, reported 82% recall for the top three results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=b66b6478-4626-431e-83e5-b65e68e44657,id=97d16771-da57-4ef1-afa0-2955d2669ceb; trace=b66b6478-4626-431e-83e5-b65e68e44657,id=cba8aaac-2786-4146-aab3-1cb9da99b3b3; trace=b66b6478-4626-431e-83e5-b65e68e44657,id=b66b6478-4626-431e-83e5-b65e68e44657\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Which maximal accuracy percent was reported for legal reserch?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
