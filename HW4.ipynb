{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e62a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.17.0)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pdf2image) (11.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb2999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting byaldi\n",
      "  Using cached Byaldi-0.0.7-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting qwen-vl-utils\n",
      "  Using cached qwen_vl_utils-0.0.14-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting colpali-engine<0.4.0,>=0.3.4 (from byaldi)\n",
      "  Using cached colpali_engine-0.3.12-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting ml-dtypes (from byaldi)\n",
      "  Using cached ml_dtypes-0.5.3-cp313-cp313-macosx_10_13_universal2.whl.metadata (8.9 kB)\n",
      "Collecting mteb==1.6.35 (from byaldi)\n",
      "  Using cached mteb-1.6.35-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting ninja (from byaldi)\n",
      "  Using cached ninja-1.13.0-py3-none-macosx_10_9_universal2.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pdf2image in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from byaldi) (1.17.0)\n",
      "Collecting srsly (from byaldi)\n",
      "  Using cached srsly-2.5.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting torch (from byaldi)\n",
      "  Using cached torch-2.9.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting datasets>=2.2.0 (from mteb==1.6.35->byaldi)\n",
      "  Using cached datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting jsonlines (from mteb==1.6.35->byaldi)\n",
      "  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting numpy (from mteb==1.6.35->byaldi)\n",
      "  Using cached numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting requests>=2.26.0 (from mteb==1.6.35->byaldi)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting scikit-learn>=1.0.2 (from mteb==1.6.35->byaldi)\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from mteb==1.6.35->byaldi)\n",
      "  Using cached scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting sentence-transformers>=2.2.0 (from mteb==1.6.35->byaldi)\n",
      "  Using cached sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting tqdm (from mteb==1.6.35->byaldi)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting rich (from mteb==1.6.35->byaldi)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pytrec-eval-terrier>=0.5.6 (from mteb==1.6.35->byaldi)\n",
      "  Using cached pytrec_eval_terrier-0.5.10-cp313-cp313-macosx_10_13_universal2.whl.metadata (1.1 kB)\n",
      "Collecting pydantic (from mteb==1.6.35->byaldi)\n",
      "  Using cached pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "Collecting typing-extensions (from mteb==1.6.35->byaldi)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting eval-type-backport (from mteb==1.6.35->byaldi)\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting av (from qwen-vl-utils)\n",
      "  Using cached av-16.0.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: packaging in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from qwen-vl-utils) (25.0)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from qwen-vl-utils) (12.0.0)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.10.23-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting peft<0.17.0,>=0.14.0 (from colpali-engine<0.4.0,>=0.3.4->byaldi)\n",
      "  Using cached peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting torch (from byaldi)\n",
      "  Using cached torch-2.7.1-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision (from colpali-engine<0.4.0,>=0.3.4->byaldi)\n",
      "  Using cached torchvision-0.24.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (5.9 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.26.0->mteb==1.6.35->byaldi)\n",
      "  Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.26.0->mteb==1.6.35->byaldi)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->mteb==1.6.35->byaldi)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.26.0->mteb==1.6.35->byaldi)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting setuptools (from torch->byaldi)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch->byaldi)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->byaldi)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch->byaldi)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.3 (from srsly->byaldi)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached pyarrow-22.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting httpx<1.0.0 (from datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting xxhash (from datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: psutil in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from peft<0.17.0,>=0.14.0->colpali-engine<0.4.0,>=0.3.4->byaldi) (7.1.2)\n",
      "Collecting accelerate>=0.21.0 (from peft<0.17.0,>=0.14.0->colpali-engine<0.4.0,>=0.3.4->byaldi)\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=1.0.2->mteb==1.6.35->byaldi)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.0.2->mteb==1.6.35->byaldi)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->byaldi)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->byaldi)\n",
      "  Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting attrs>=19.2.0 (from jsonlines->mteb==1.6.35->byaldi)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->mteb==1.6.35->byaldi)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic->mteb==1.6.35->byaldi)\n",
      "  Using cached pydantic_core-2.41.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic->mteb==1.6.35->byaldi)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->mteb==1.6.35->byaldi)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from rich->mteb==1.6.35->byaldi) (2.19.2)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from colpali-engine<0.4.0,>=0.3.4->byaldi)\n",
      "  Using cached torchvision-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "  Using cached torchvision-0.22.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached aiohttp-3.13.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->mteb==1.6.35->byaldi)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from pandas->datasets>=2.2.0->mteb==1.6.35->byaldi) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.2.0->mteb==1.6.35->byaldi) (1.17.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets>=2.2.0->mteb==1.6.35->byaldi)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached Byaldi-0.0.7-py3-none-any.whl (21 kB)\n",
      "Using cached mteb-1.6.35-py3-none-any.whl (517 kB)\n",
      "Using cached qwen_vl_utils-0.0.14-py3-none-any.whl (8.1 kB)\n",
      "Using cached colpali_engine-0.3.12-py3-none-any.whl (73 kB)\n",
      "Using cached transformers-4.53.3-py3-none-any.whl (10.8 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached regex-2025.10.23-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached torch-2.7.1-cp313-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached av-16.0.1-cp313-cp313-macosx_14_0_arm64.whl (21.7 MB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached ml_dtypes-0.5.3-cp313-cp313-macosx_10_13_universal2.whl (663 kB)\n",
      "Using cached ninja-1.13.0-py3-none-macosx_10_9_universal2.whl (310 kB)\n",
      "Using cached srsly-2.5.1-cp313-cp313-macosx_11_0_arm64.whl (632 kB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "Using cached pytrec_eval_terrier-0.5.10-cp313-cp313-macosx_10_13_universal2.whl (137 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Using cached pydantic_core-2.41.4-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached torchvision-0.22.1-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-macosx_12_0_arm64.whl (34.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m917.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohttp-3.13.2-cp313-cp313-macosx_11_0_arm64.whl (489 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl (49 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl (46 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl (93 kB)\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, sniffio, setuptools, safetensors, regex, pyyaml, pyarrow, propcache, numpy, ninja, networkx, multidict, mdurl, MarkupSafe, joblib, idna, hf-xet, h11, fsspec, frozenlist, filelock, eval-type-backport, dill, charset_normalizer, certifi, catalogue, av, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspection, srsly, scipy, requests, pydantic-core, pandas, multiprocess, ml-dtypes, markdown-it-py, jsonlines, jinja2, httpcore, anyio, aiosignal, torch, scikit-learn, rich, qwen-vl-utils, pytrec-eval-terrier, pydantic, huggingface-hub, httpx, aiohttp, torchvision, tokenizers, accelerate, transformers, datasets, sentence-transformers, peft, mteb, colpali-engine, byaldi\n",
      "Successfully installed MarkupSafe-3.0.3 accelerate-1.11.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.11.0 attrs-25.4.0 av-16.0.1 byaldi-0.0.7 catalogue-2.0.10 certifi-2025.10.5 charset_normalizer-3.4.4 colpali-engine-0.3.12 datasets-4.3.0 dill-0.4.0 eval-type-backport-0.2.2 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.9.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 joblib-1.5.2 jsonlines-4.0.0 markdown-it-py-4.0.0 mdurl-0.1.2 ml-dtypes-0.5.3 mpmath-1.3.0 mteb-1.6.35 multidict-6.7.0 multiprocess-0.70.16 networkx-3.5 ninja-1.13.0 numpy-2.3.4 pandas-2.3.3 peft-0.16.0 propcache-0.4.1 pyarrow-22.0.0 pydantic-2.12.3 pydantic-core-2.41.4 pytrec-eval-terrier-0.5.10 pytz-2025.2 pyyaml-6.0.3 qwen-vl-utils-0.0.14 regex-2025.10.23 requests-2.32.5 rich-14.2.0 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.3 sentence-transformers-5.1.2 setuptools-80.9.0 sniffio-1.3.1 srsly-2.5.1 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.4 torch-2.7.1 torchvision-0.22.1 tqdm-4.67.1 transformers-4.53.3 typing-extensions-4.15.0 typing-inspection-0.4.2 tzdata-2025.2 urllib3-2.5.0 xxhash-3.6.0 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install byaldi qwen-vl-utils transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf41cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.8.1)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from seaborn) (2.3.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from seaborn) (2.3.3)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Using cached matplotlib-3.10.7-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached fonttools-4.60.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.0.0)\n",
      "Collecting pyparsing>=3 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yurii/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached matplotlib-3.10.7-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Using cached kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5 seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install einops seaborn ##not sure if it is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e8515a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def convert_pdf_to_images(pdf_path):\n",
    "    \"\"\"Converts a local PDF file into a list of PIL.Image objects.\"\"\"\n",
    "    print(f\"Converting {pdf_path} to images...\")\n",
    "    images = convert_from_path(pdf_path)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9c44933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ../Databases for GenAI.pdf to images...\n"
     ]
    }
   ],
   "source": [
    "pdf_file_path = \"../Databases for GenAI.pdf\"\n",
    "page_images = convert_pdf_to_images(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f3c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "\n",
    "\n",
    "#vison language models\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "from io import BytesIO\n",
    "#import matplotlib.pyplot as plt\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6171b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 31300.78it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.94it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vidore/colpali-v1.2\"\n",
    "processor = ColPaliProcessor.from_pretrained(model_name)\n",
    "\n",
    "model = ColPali.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ce3134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [40:14<00:00, 241.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 20 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_document_embeddings(page_images, model, processor, batch_size=2):\n",
    "    \n",
    "    all_embeddings_with_metadata = []\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "            dataset=page_images,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=lambda x: processor.process_images(x),\n",
    "        )\n",
    "\n",
    "    page_counter = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            batch_embeddings = model(**batch)\n",
    "            batch_embeddings = list(torch.unbind(batch_embeddings.to(\"cpu\")))\n",
    "\n",
    "            for embedding in batch_embeddings:\n",
    "                all_embeddings_with_metadata.append({\n",
    "                    \"embedding\": embedding,\n",
    "                    \"page_id\": page_counter,\n",
    "                })\n",
    "                page_counter += 1\n",
    "\n",
    "    return all_embeddings_with_metadata\n",
    "\n",
    "\n",
    "ds = create_document_embeddings(page_images, model, processor, batch_size=2)\n",
    "print(f\"Generated embeddings for {len(ds)} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c665fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the list of embeddings to a file just in case\n",
    "with open('ds.pkl', 'wb') as f:\n",
    "    pickle.dump(ds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd501e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the list\n",
    "# with open('ds.pkl', 'rb') as f:\n",
    "#     ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0596ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(query, processor, model, ds, all_images, top_k=5):\n",
    "    \n",
    "    # Process the query and move to model's device\n",
    "    batch_queries = processor.process_queries([query]).to(model.device)\n",
    "\n",
    "    # Forward pass to get query embeddings\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = model(**batch_queries)\n",
    "\n",
    "    # Extract embeddings from ds for scoring\n",
    "    document_embeddings = torch.stack([entry[\"embedding\"] for entry in ds])\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    scores = processor.score_multi_vector(query_embeddings, document_embeddings)\n",
    "    score_values = scores[0].tolist()  # Extract similarity scores as a list\n",
    "\n",
    "    # Get top-k indices of the most relevant embeddings\n",
    "    top_indices = scores[0].topk(top_k).indices.tolist()\n",
    "\n",
    "    # Retrieve corresponding images and metadata\n",
    "    retrieved_results = []\n",
    "    for idx in top_indices:\n",
    "        entry = ds[idx]\n",
    "        page_id = entry[\"page_id\"]\n",
    "        image = all_images[page_id]\n",
    "\n",
    "        # Add score to each result\n",
    "        retrieved_results.append({\n",
    "            \"page_id\": page_id,\n",
    "            \"image\": image,\n",
    "            \"score\": score_values[idx],  # Add similarity score\n",
    "        })\n",
    "\n",
    "        return retrieved_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0773138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 12.53it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "vl_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16).to(device).eval()\n",
    "min_pixels = 224 * 224\n",
    "max_pixels = 1024 * 1024\n",
    "vl_model_processor = Qwen2VLProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5d0613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(vl_model,vl_model_processor,query,processor, model, ds, page_images,max_new_tokens=500):\n",
    "    \n",
    "    retrieved_results = get_results(\n",
    "        query = query,\n",
    "        processor=processor,\n",
    "        model=model,\n",
    "        ds=ds,\n",
    "        all_images=page_images,\n",
    "        top_k=2,\n",
    "    )\n",
    "    grouped_images = [result[\"image\"] for result in retrieved_results]\n",
    "    chat_template = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"image\", \"image\": image} for image in grouped_images]\n",
    "            + [{\"type\": \"text\", \"text\": query}],\n",
    "        }\n",
    "    ]\n",
    "    # process the template\n",
    "    text = vl_model_processor.apply_chat_template(chat_template, tokenize=False, \n",
    "                                              add_generation_prompt=True)\n",
    "\n",
    "    image_inputs, _ = process_vision_info(chat_template)\n",
    "    inputs = vl_model_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    # generate model output\n",
    "    generated_ids = vl_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = vl_model_processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f031320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q langchain-google-genai langchain-qdrant langchain-text-splitters langchain-community langgraph langchain\n",
    "## there casn be issues with versions, I struggled a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"False\"  ##true does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b271fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from pydantic import ConfigDict\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a92105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3eccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualPDFRetriever(BaseRetriever):\n",
    "    vl_model: object\n",
    "    vl_model_processor: object\n",
    "    processor: object\n",
    "    model: object\n",
    "    ds: list\n",
    "    page_images: list\n",
    "    max_new_tokens: int = 500\n",
    "    \n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def __init__(self, vl_model,vl_model_processor,processor, model, ds, page_images,max_new_tokens=500):\n",
    "        super().__init__(\n",
    "            vl_model=vl_model,\n",
    "            vl_model_processor=vl_model_processor,\n",
    "            processor=processor,\n",
    "            model=model,\n",
    "            ds=ds,\n",
    "            page_images=page_images,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        doc = Document(\n",
    "            page_content=generate_answers(self.vl_model,self.vl_model_processor,query,self.processor, self.model, self.ds, self.page_images, self.max_new_tokens),\n",
    "            metadata={\n",
    "                \"source\": \"PDF Document\",\n",
    "            }\n",
    "        )\n",
    "        return [doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77808439",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_retriever = VisualPDFRetriever(\n",
    "    vl_model,vl_model_processor, processor, model, ds, page_images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaac8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MY_API_KEY = \"AIzaSyBYCquvMp6UAaODqfjRu0lppEUUwIa4foA\"\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\"\n",
    "                                          , google_api_key=MY_API_KEY)\n",
    "ex_emb = embeddings.embed_query(\"sample text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install moviepy librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7cee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import *\n",
    "\n",
    "video_path = \"../2 part Databases for GenAI.mp4\"\n",
    "audio_path = \"../extracted_audio.mp3\"\n",
    "\n",
    "# --- Part 1: Extract Audio using MoviePy ---\n",
    "print(f\"Extracting audio from {video_path}...\")\n",
    "\n",
    "# Load the video file\n",
    "video = VideoFileClip(video_path)\n",
    "\n",
    "# Extract the audio and write it to a new file\n",
    "video.audio.write_audiofile(audio_path)\n",
    "\n",
    "print(f\"Audio extracted and saved to {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d28ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6624f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "audio_array, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "transcription = transcriber(audio_array, return_timestamps=True)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a8046c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../audio_transcript.txt\", \"w\") as text_file:\n",
    "    text_file.write(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02281d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"../audio_transcript.txt\") \n",
    "transcript = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85f06e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "vector_size = len(ex_emb)\n",
    "\n",
    "if not client.collection_exists(\"test\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"test\",\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "    )\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"test\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c312b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\"\n",
    "                      , google_api_key=MY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a7776d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(transcript)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a731832",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRetriever(BaseRetriever):\n",
    "    vector_store: object\n",
    "    \n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def __init__(self, vector_store: object):\n",
    "        super().__init__(\n",
    "            vector_store=vector_store\n",
    "        )\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Translates a string query into a graph invocation and returns documents.\n",
    "        \"\"\"\n",
    "        # print(f\"\\n[StateGraphRetriever]: Processing query: {query}\")\n",
    "        \n",
    "        # 1. CONSTRUCT THE INPUT STATE\n",
    "        initial_state = {\n",
    "            \"question\":  query,\n",
    "             \"context\": \"\", \n",
    "        }\n",
    "\n",
    "        # 2. INVOKE THE GRAPH\n",
    "        # We run the graph to completion\n",
    "        final_state = self.vector_store.similarity_search(initial_state[\"question\"])\n",
    "\n",
    "        return final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd42aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_retriever = TextRetriever(\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302bb23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.retrievers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EnsembleRetriever\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.retrievers'"
     ]
    }
   ],
   "source": [
    "# from langchain.retrievers.ensemble import EnsembleRetriever ## cannot import due to libraries dependency issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efccb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Optional, Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.callbacks import Callbacks\n",
    "\n",
    "# --- Created this with help of GEMINI instead of EnsembleRetriever ---\n",
    "\n",
    "class ManualEnsembleRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    A manual replacement for EnsembleRetriever that combines\n",
    "    results using Reciprocal Rank Fusion (RRF).\n",
    "    \n",
    "    It only depends on 'langchain-core'.\n",
    "    \"\"\"\n",
    "    \n",
    "    retrievers: List[BaseRetriever]\n",
    "    weights: Optional[List[float]] = None\n",
    "    c: int = 60  # RRF constant, k=60 is the default\n",
    "\n",
    "    def __init__(self, *, retrievers: List[BaseRetriever], weights: Optional[List[float]] = None, **kwargs: Any):\n",
    "        \"\"\"Initialize the retriever.\"\"\"\n",
    "        super().__init__(retrievers=retrievers, weights=weights, **kwargs)\n",
    "        \n",
    "        if self.weights is None:\n",
    "            self.weights = [1.0] * len(self.retrievers)\n",
    "        \n",
    "        if len(self.retrievers) != len(self.weights):\n",
    "            raise ValueError(\n",
    "                f\"Number of retrievers ({len(self.retrievers)}) must \"\n",
    "                f\"equal number of weights ({len(self.weights)})\"\n",
    "            )\n",
    "\n",
    "    # def _get_relevant_documents( ## there are more sophisticated versions with config and callbacks\n",
    "    #     self, query: str, *, config: Optional[RunnableConfig] = None, **kwargs: Any\n",
    "    # ) -> List[Document]:\n",
    "    #     \"\"\"Sync version of the ensemble retrieval.\"\"\"\n",
    "    #     return asyncio.run(self._aget_relevant_documents(query, config=config, **kwargs)) ## async version does not work well\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, config: Optional[RunnableConfig] = None, **kwargs: Any\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        print(\"query:\", query)\n",
    "\n",
    "        # Get callbacks for managing async calls\n",
    "        callbacks = config.get(\"callbacks\") if config else None\n",
    "        rrf_scores = defaultdict(float)\n",
    "        doc_store = {}\n",
    "        \n",
    "        # 1. Process each retriever's results one by one (synchronously)\n",
    "        for retriever, weight in zip(self.retrievers, self.weights):\n",
    "            \n",
    "            # Call the synchronous .invoke() method\n",
    "            doc_list = retriever.invoke(query, config=config, **kwargs)\n",
    "            \n",
    "            for i, doc in enumerate(doc_list):\n",
    "                # Use page_content as a unique key.\n",
    "                # A metadata ID is better if you have one.\n",
    "                doc_key = doc.page_content  \n",
    "                \n",
    "                if doc_key not in doc_store:\n",
    "                    doc_store[doc_key] = doc\n",
    "                \n",
    "                # 2. Calculate RRF score\n",
    "                rank = i + 1  # Ranks are 1-based\n",
    "                rrf_scores[doc_key] += weight * (1.0 / (self.c + rank))\n",
    "\n",
    "        # 4. Sort documents by their combined RRF score\n",
    "        sorted_doc_keys = sorted(\n",
    "            rrf_scores.keys(), \n",
    "            key=lambda k: rrf_scores[k], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # 5. Return the final, sorted list of Document objects\n",
    "        final_sorted_docs = [doc_store[key] for key in sorted_doc_keys]\n",
    "        \n",
    "        return final_sorted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "592b9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = ManualEnsembleRetriever(\n",
    "    retrievers=[text_retriever, visual_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8593a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = ensemble_retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \n",
    "                              \"context\": state[\"context\"]}) \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0ded4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_chat(question: str):\n",
    "    res = graph.invoke({\"question\": question})\n",
    "    return res[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "83e48bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Which maximal accuracy percent was reported for legal research?\n",
      "The maximal accuracy percent reported for legal research is 87%. This was achieved using 1.4 million law documents with Gemini embedding.\n"
     ]
    }
   ],
   "source": [
    "print(my_chat(\"Which maximal accuracy percent was reported for legal research?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "11618fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Who is the Head of Cloud Platforms & AI Director?\n",
      "The Head of Cloud Platforms & AI Director is Maksym Lypivskyi.\n",
      "query: Why is hybrid search better than vector-only search?\n",
      "Hybrid search is better than vector-only search because it combines the strengths of both approaches, such as the speed of vector-based search with the accuracy of BM25. This makes it a more efficient and effective search method. Researchers have found that a hybrid system approach, combining vector search with BM25, can increase accuracy by more than 10%.\n",
      "query: What are the production DO's for RAG?\n",
      "The production DO's for RAG include Query Embedding, Vector Database, Candidate Selection, Prompt with Context, LLM, and Output Message. These steps convert the user's query into a vector, store candidate responses, select relevant ones, generate a prompt for the LLM, and send the final output. Additionally, the user can take control over the world by interacting with external APIs or taking actions.\n"
     ]
    }
   ],
   "source": [
    "print(my_chat(\"Who is the Head of Cloud Platforms & AI Director?\"))\n",
    "print(my_chat(\"Why is hybrid search better than vector-only search?\"))\n",
    "print(my_chat(\"What are the production DO's for RAG?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
